{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f102c9",
   "metadata": {},
   "source": [
    "# Getting Query from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c925d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading URL 1/3: https://pubmed.ncbi.nlm.nih.gov/27242579/...\n",
      "-> Saved C:\\Users\\User\\Desktop\\Psychoinformatics Neuroinformatics\\ETL_HW\\html_outputs\\output_1.html (134,094 bytes)\n",
      "\n",
      "Downloading URL 2/3: https://pubmed.ncbi.nlm.nih.gov/32457675/...\n",
      "-> Saved C:\\Users\\User\\Desktop\\Psychoinformatics Neuroinformatics\\ETL_HW\\html_outputs\\output_2.html (138,335 bytes)\n",
      "\n",
      "Downloading URL 3/3: https://pubmed.ncbi.nlm.nih.gov/32528365/...\n",
      "-> Saved C:\\Users\\User\\Desktop\\Psychoinformatics Neuroinformatics\\ETL_HW\\html_outputs\\output_3.html (128,509 bytes)\n",
      "\n",
      "All downloads complete.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# 1. Create a list of all the URLs you want to download\n",
    "URLS = [\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/27242579/\",\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/32457675/\",\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/32528365/\"\n",
    "]\n",
    "\n",
    "# Set browser-like headers to avoid being blocked by the site\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://pubmed.ncbi.nlm.nih.gov/\",\n",
    "}\n",
    "\n",
    "# Create a directory to store the output files\n",
    "output_dir = Path(\"html_outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Use a single session for all requests for efficiency\n",
    "with requests.Session() as s:\n",
    "    s.headers.update(headers)\n",
    "    \n",
    "    # 2. Loop through each URL in the list\n",
    "    for i, url in enumerate(URLS):\n",
    "        print(f\"Downloading URL {i+1}/{len(URLS)}: {url[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # 3. Create a unique filename for each URL\n",
    "            outfile = output_dir / f\"output_{i+1}.html\"\n",
    "\n",
    "            resp = s.get(url, timeout=30)\n",
    "            resp.raise_for_status()  # raise an error for non-200 responses\n",
    "\n",
    "            # Use server-provided encoding when available; default to utf-8\n",
    "            if not resp.encoding:\n",
    "                resp.encoding = \"utf-8\"\n",
    "                \n",
    "            outfile.write_text(resp.text, encoding=resp.encoding)\n",
    "\n",
    "            print(f\"-> Saved {outfile.resolve()} ({outfile.stat().st_size:,} bytes)\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"!! Failed to download URL {i+1}. Error: {e}\\n\")\n",
    "        \n",
    "        # Optional: Add a small delay to be respectful to the server\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"All downloads complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1277d0a",
   "metadata": {},
   "source": [
    "# Extracting PMID from the HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5354c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 HTML files to process in 'html_outputs'...\n",
      "\n",
      "Processing: After the Honeymoon.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Processing: Attractive Alternative Partners.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Processing: Lucky Guy in Love.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Processing: output_1.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Processing: output_2.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Processing: output_3.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Source File               | PMID\n",
      "--------------------------|--------\n",
      "After the Honeymoon.html  | 32457675\n",
      "Attractive Alternative Partners.html | 32528365\n",
      "Lucky Guy in Love.html    | 27242579\n",
      "output_1.html             | 27242579\n",
      "output_2.html             | 32457675\n",
      "output_3.html             | 32528365\n",
      "\n",
      "========================================\n",
      "Processing complete!\n",
      "Total PMIDs extracted: 6\n",
      "['32457675', '32528365', '27242579', '27242579', '32457675', '32528365']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define the directory containing your HTML files\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "\n",
    "# 2. Create a list to store the results as dictionaries\n",
    "extraction_results = []\n",
    "\n",
    "# 3. Find all .html files in the directory and loop through them\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "print(f\"Found {len(html_files)} HTML files to process in '{INPUT_DIR}'...\\n\")\n",
    "\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    \n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Find the specific meta tag\n",
    "    meta = soup.find('meta', attrs={'name': 'log_displayeduids'})\n",
    "    \n",
    "    if meta:\n",
    "        pmids_str = meta.get('content', '')\n",
    "        if pmids_str:\n",
    "            # Split the string by comma to get individual IDs\n",
    "            pmids = pmids_str.split(',')\n",
    "            \n",
    "            # For each PMID found, add it to our results with its source file\n",
    "            for pmid in pmids:\n",
    "                cleaned_pmid = pmid.strip()\n",
    "                if cleaned_pmid: # Ensure it's not an empty string\n",
    "                    extraction_results.append({\n",
    "                        'source_file': html_file.name,\n",
    "                        'pmid': cleaned_pmid\n",
    "                    })\n",
    "            \n",
    "            print(f\"  -> Found {len(pmids)} PMIDs.\\n\")\n",
    "        else:\n",
    "            print(\"  -> Meta tag found, but it has no content.\\n\")\n",
    "    else:\n",
    "        print(f\"  -> WARNING: Could not find the 'log_displayeduids' meta tag in this file.\\n\")\n",
    "\n",
    "## **Extraction Results**\n",
    "\n",
    "# 4. Print the final results directly to the console\n",
    "print(f\"{'Source File':<25} | {'PMID'}\")\n",
    "print(f\"{'-'*25}-|--------\")\n",
    "\n",
    "if not extraction_results:\n",
    "    print(\"No PMIDs were found.\")\n",
    "else:\n",
    "    for item in extraction_results:\n",
    "        print(f\"{item['source_file']:<25} | {item['pmid']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Total PMIDs extracted: {len(extraction_results)}\")\n",
    "extraction_results[:3]\n",
    "pmids_only = [item['pmid'] for item in extraction_results]\n",
    "print(pmids_only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ea673",
   "metadata": {},
   "source": [
    "# Extracting PMC ID from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "978bfa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 HTML files to process in 'html_outputs'...\n",
      "\n",
      "Processing: After the Honeymoon.html\n",
      "  -> Found PMC ID: PMC7223160\n",
      "\n",
      "Processing: Attractive Alternative Partners.html\n",
      "  -> Found PMC ID: PMC7264388\n",
      "\n",
      "Processing: Lucky Guy in Love.html\n",
      "  -> Found PMC ID: PMC4863427\n",
      "\n",
      "Processing: output_1.html\n",
      "  -> Found PMC ID: PMC4863427\n",
      "\n",
      "Processing: output_2.html\n",
      "  -> Found PMC ID: PMC7223160\n",
      "\n",
      "Processing: output_3.html\n",
      "  -> Found PMC ID: PMC7264388\n",
      "\n",
      "Source File               | PMC ID\n",
      "--------------------------|-----------\n",
      "After the Honeymoon.html  | PMC7223160\n",
      "Attractive Alternative Partners.html | PMC7264388\n",
      "Lucky Guy in Love.html    | PMC4863427\n",
      "output_1.html             | PMC4863427\n",
      "output_2.html             | PMC7223160\n",
      "output_3.html             | PMC7264388\n",
      "\n",
      "========================================\n",
      "Processing complete!\n",
      "Total files with PMC IDs: 6\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define the directory containing your HTML files\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "\n",
    "# 2. Create a list to store the results\n",
    "pmc_results = []\n",
    "\n",
    "# 3. Find all .html files in the directory and loop through them\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "print(f\"Found {len(html_files)} HTML files to process in '{INPUT_DIR}'...\\n\")\n",
    "\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    \n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Find the meta tag with name=\"keywords\"\n",
    "    keywords_meta_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    \n",
    "    pmc_id_found = False\n",
    "    if keywords_meta_tag:\n",
    "        # Get the content string, which contains pmid, pmcid, doi, etc.\n",
    "        content_str = keywords_meta_tag.get('content', '')\n",
    "        \n",
    "        # Split the string by commas to get individual parts\n",
    "        content_parts = content_str.split(',')\n",
    "        \n",
    "        # Loop through the parts to find the one starting with \"PMC\"\n",
    "        for part in content_parts:\n",
    "            # .strip() removes any leading/trailing whitespace\n",
    "            cleaned_part = part.strip()\n",
    "            if cleaned_part.startswith('PMC'):\n",
    "                pmc_results.append({\n",
    "                    'source_file': html_file.name,\n",
    "                    'pmc_id': cleaned_part\n",
    "                })\n",
    "                print(f\"  -> Found PMC ID: {cleaned_part}\\n\")\n",
    "                pmc_id_found = True\n",
    "                break # Stop searching once the PMC ID is found\n",
    "    \n",
    "    # If the loop finishes and no PMC ID was found\n",
    "    if not pmc_id_found:\n",
    "        print(f\"  -> No PMC ID found in this file.\\n\")\n",
    "\n",
    "## **Extraction Results**\n",
    "\n",
    "# 4. Print the final results directly to the console\n",
    "print(f\"{'Source File':<25} | {'PMC ID'}\")\n",
    "print(f\"{'-'*25}-|-----------\")\n",
    "\n",
    "if not pmc_results:\n",
    "    print(\"No PMC IDs were found in any of the files.\")\n",
    "else:\n",
    "    for item in pmc_results:\n",
    "        print(f\"{item['source_file']:<25} | {item['pmc_id']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Total files with PMC IDs: {len(pmc_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56969471",
   "metadata": {},
   "source": [
    "# Extracting the keywords from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d0f278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 HTML files to process in 'html_outputs'...\n",
      "\n",
      "Processing: After the Honeymoon.html\n",
      "  -> Found 5 keywords.\n",
      "\n",
      "Processing: Attractive Alternative Partners.html\n",
      "  -> Found 5 keywords.\n",
      "\n",
      "Processing: Lucky Guy in Love.html\n",
      "  -> Found 6 keywords.\n",
      "\n",
      "Processing: output_1.html\n",
      "  -> Found 6 keywords.\n",
      "\n",
      "Processing: output_2.html\n",
      "  -> Found 5 keywords.\n",
      "\n",
      "Processing: output_3.html\n",
      "  -> Found 5 keywords.\n",
      "\n",
      "Source File               | Keywords\n",
      "--------------------------|-----------\n",
      "After the Honeymoon.html  | dopamine, fMRI, marriage, pair-bonds, romantic love\n",
      "Attractive Alternative Partners.html | attention to alternatives, close relationship, romantic love, self-expansion, social neuroscience\n",
      "Lucky Guy in Love.html    | AI, MPFC, aMCC, fMRI, intrasexual competition, pain empathy\n",
      "output_1.html             | AI, MPFC, aMCC, fMRI, intrasexual competition, pain empathy\n",
      "output_2.html             | dopamine, fMRI, marriage, pair-bonds, romantic love\n",
      "output_3.html             | attention to alternatives, close relationship, romantic love, self-expansion, social neuroscience\n",
      "\n",
      "========================================\n",
      "Processing complete!\n",
      "Total files with keywords: 6\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# 1. Define the directory containing your HTML files\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "\n",
    "# 2. Create a list to store the results\n",
    "keyword_results = []\n",
    "\n",
    "# 3. Find all .html files and loop through them\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "print(f\"Found {len(html_files)} HTML files to process in '{INPUT_DIR}'...\\n\")\n",
    "\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    \n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Find the <strong> tag that contains the text \"Keywords:\"\n",
    "    # We use a regex with re.IGNORECASE to match \"Keywords:\" or \"keywords:\"\n",
    "    keyword_tag = soup.find('strong', string=re.compile(r'Keywords:', re.IGNORECASE))\n",
    "    \n",
    "    keywords_found = False\n",
    "    if keyword_tag:\n",
    "        # The keywords are in the text node immediately following the <strong> tag\n",
    "        next_element = keyword_tag.next_sibling\n",
    "        \n",
    "        # Check if the next element is actually text (a NavigableString)\n",
    "        if next_element and isinstance(next_element, NavigableString):\n",
    "            # .strip() removes whitespace, .rstrip('.') removes the final period\n",
    "            keyword_str = next_element.strip().rstrip('.')\n",
    "            \n",
    "            # Split the string by semicolon and clean up each keyword\n",
    "            keywords = [kw.strip() for kw in keyword_str.split(';')]\n",
    "            \n",
    "            keyword_results.append({\n",
    "                'source_file': html_file.name,\n",
    "                'keywords': keywords\n",
    "            })\n",
    "            print(f\"  -> Found {len(keywords)} keywords.\\n\")\n",
    "            keywords_found = True\n",
    "\n",
    "    if not keywords_found:\n",
    "        print(f\"  -> No keywords section found in this file.\\n\")\n",
    "\n",
    "## **Extraction Results**\n",
    "\n",
    "# 4. Print the final results\n",
    "print(f\"{'Source File':<25} | {'Keywords'}\")\n",
    "print(f\"{'-'*25}-|-----------\")\n",
    "\n",
    "if not keyword_results:\n",
    "    print(\"No keywords were found in any of the files.\")\n",
    "else:\n",
    "    for item in keyword_results:\n",
    "        # ', '.join() converts the list of keywords into a nice string for printing\n",
    "        keywords_str = ', '.join(item['keywords'])\n",
    "        print(f\"{item['source_file']:<25} | {keywords_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Total files with keywords: {len(keyword_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626d8af",
   "metadata": {},
   "source": [
    "# LM Studio API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "776b549b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": [\n",
      "        {\n",
      "            \"id\": \"google/gemma-3-4b\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": \"text-embedding-nomic-embed-text-v1.5\",\n",
      "            \"object\": \"model\",\n",
      "            \"owned_by\": \"organization_owner\"\n",
      "        }\n",
      "    ],\n",
      "    \"object\": \"list\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# See all the available models:\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:1234/v1/models\"\n",
    "response = requests.get(url)\n",
    "models = response.json()\n",
    "print(json.dumps(models, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "040d714f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-ghyy09tkqoto8s43cnapfi\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1758100954,\n",
      "    \"model\": \"google/gemma-3-4b\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"1 + 1 = 2\\n\",\n",
      "                \"tool_calls\": []\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 20,\n",
      "        \"completion_tokens\": 9,\n",
      "        \"total_tokens\": 29\n",
      "    },\n",
      "    \"stats\": {},\n",
      "    \"system_fingerprint\": \"google/gemma-3-4b\"\n",
      "}\n",
      "1 + 1 = 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma-3-4b-it-qat\",   # must match one of your loaded models\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"1+1=?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "data = response.json()\n",
    "\n",
    "print(json.dumps(data, indent=4))  # print whole response to inspect\n",
    "print(data[\"choices\"][0][\"message\"][\"content\"])  # the assistant's reply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad7a94a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-rnjps7u5e8otzryqkdufzl\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1758101396,\n",
      "    \"model\": \"google/gemma-3-4b\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"This image is a table from a research paper, likely related to neuroscience or psychology. Here\\u2019s a breakdown of what it shows:\\n\\n*   **Title:** \\\"Brain responses replicated at Times 1 and 2\\\" - This suggests the study looked at brain activity changes over two time points (likely before and after an intervention).\\n*   **Columns:** The table presents data for different brain regions, along with statistical measures. Let's break down the columns:\\n\\n    *   **Brain Region:** Lists specific areas of the brain being studied (e.g., SN lateral, Paraentral lobule, Inferior frontal gyrus).\\n    *   **x, y:** These likely represent coordinates within the brain region \\u2013 potentially used for spatial analysis.\\n    *   **z:** This is probably a standardized value related to activation levels.\\n    *   **T:** Represents the t-statistic, which is used in statistical tests (specifically, t-tests) to determine if there's a significant difference between groups.\\n    *   **p:**  This is the p-value, indicating the probability of observing the data if there were no real effect. A smaller p-value (typically <0.05) suggests stronger evidence for a statistically significant result.\\n*   **Rows:** The table shows results for \\\"Brain responses replicated\\\" at Times 1 and 2. This means that the observed changes in brain activity were consistent across both time points, suggesting stability.\\n*   **Sections:** The table is divided into sections based on different types of data:\\n\\n    *   **RQI Activations:**  This section shows activation levels for specific brain regions.\\n    *   **Whole-brain Deactivations:** This section indicates which areas showed decreased activity (deactivation) during the study.\\n*   **References:** The bottom line provides references to other research papers cited in the study.\\n\\n**In essence, this table summarizes statistical findings from a study investigating brain activity changes related to emotional well-being (indicated by \\\"Eros scores\\\") in newlywed couples.** It shows which brain regions showed significant activation or deactivation and whether those patterns were consistent across two time points.\\n\\nDo you want me to delve into any specific part of the table, such as:\\n\\n*   What a particular t-statistic means?\\n*   The significance of the p-values?\\n*   The meaning of the brain regions listed?\",\n",
      "                \"tool_calls\": []\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 24,\n",
      "        \"completion_tokens\": 502,\n",
      "        \"total_tokens\": 526\n",
      "    },\n",
      "    \"stats\": {},\n",
      "    \"system_fingerprint\": \"google/gemma-3-4b\"\n",
      "}\n",
      "This image is a table from a research paper, likely related to neuroscience or psychology. Here’s a breakdown of what it shows:\n",
      "\n",
      "*   **Title:** \"Brain responses replicated at Times 1 and 2\" - This suggests the study looked at brain activity changes over two time points (likely before and after an intervention).\n",
      "*   **Columns:** The table presents data for different brain regions, along with statistical measures. Let's break down the columns:\n",
      "\n",
      "    *   **Brain Region:** Lists specific areas of the brain being studied (e.g., SN lateral, Paraentral lobule, Inferior frontal gyrus).\n",
      "    *   **x, y:** These likely represent coordinates within the brain region – potentially used for spatial analysis.\n",
      "    *   **z:** This is probably a standardized value related to activation levels.\n",
      "    *   **T:** Represents the t-statistic, which is used in statistical tests (specifically, t-tests) to determine if there's a significant difference between groups.\n",
      "    *   **p:**  This is the p-value, indicating the probability of observing the data if there were no real effect. A smaller p-value (typically <0.05) suggests stronger evidence for a statistically significant result.\n",
      "*   **Rows:** The table shows results for \"Brain responses replicated\" at Times 1 and 2. This means that the observed changes in brain activity were consistent across both time points, suggesting stability.\n",
      "*   **Sections:** The table is divided into sections based on different types of data:\n",
      "\n",
      "    *   **RQI Activations:**  This section shows activation levels for specific brain regions.\n",
      "    *   **Whole-brain Deactivations:** This section indicates which areas showed decreased activity (deactivation) during the study.\n",
      "*   **References:** The bottom line provides references to other research papers cited in the study.\n",
      "\n",
      "**In essence, this table summarizes statistical findings from a study investigating brain activity changes related to emotional well-being (indicated by \"Eros scores\") in newlywed couples.** It shows which brain regions showed significant activation or deactivation and whether those patterns were consistent across two time points.\n",
      "\n",
      "Do you want me to delve into any specific part of the table, such as:\n",
      "\n",
      "*   What a particular t-statistic means?\n",
      "*   The significance of the p-values?\n",
      "*   The meaning of the brain regions listed?\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "url = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "# Read image and encode to base64\n",
    "with open(\"After the Honeymoon Table 3.jpg\", \"rb\") as f:\n",
    "    image_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma-3-4b-it-qat\",   # <-- make sure this is a vision-capable model\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "data = response.json()\n",
    "\n",
    "print(json.dumps(data, indent=4))  # inspect full response\n",
    "print(data[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15690118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"id\": \"chatcmpl-66aobvpvtsag1yl8c5vpe6\",\n",
      "    \"object\": \"chat.completion\",\n",
      "    \"created\": 1758104520,\n",
      "    \"model\": \"google/gemma-3-4b\",\n",
      "    \"choices\": [\n",
      "        {\n",
      "            \"index\": 0,\n",
      "            \"message\": {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Here's the extracted data from the table:\\n\\n| Region                | X  | Y  | Z  |\\n|-----------------------|----|----|----|\\n| SN, lateral          | 15 | -15 | -12 |\\n| Parainterial lobule   | -6 | -24 | 57 |\\n| Inferior frontal gyrus | 54 | 21 | 3  |\",\n",
      "                \"tool_calls\": []\n",
      "            },\n",
      "            \"logprobs\": null,\n",
      "            \"finish_reason\": \"stop\"\n",
      "        }\n",
      "    ],\n",
      "    \"usage\": {\n",
      "        \"prompt_tokens\": 48,\n",
      "        \"completion_tokens\": 96,\n",
      "        \"total_tokens\": 144\n",
      "    },\n",
      "    \"stats\": {},\n",
      "    \"system_fingerprint\": \"google/gemma-3-4b\"\n",
      "}\n",
      "Here's the extracted data from the table:\n",
      "\n",
      "| Region                | X  | Y  | Z  |\n",
      "|-----------------------|----|----|----|\n",
      "| SN, lateral          | 15 | -15 | -12 |\n",
      "| Parainterial lobule   | -6 | -24 | 57 |\n",
      "| Inferior frontal gyrus | 54 | 21 | 3  |\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "\n",
    "url = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "# Read image and encode to base64\n",
    "with open(\"After the Honeymoon Table 3.jpg\", \"rb\") as f:\n",
    "    image_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"gemma-3-4b-it-qat\",   # <-- make sure this is a vision-capable model\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"This table contains XYZ coordinates of brain regions from a neuroscience study. Please extract the 'Region', 'X', 'Y', 'Z' columns only.\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_base64}\"\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "data = response.json()\n",
    "\n",
    "print(json.dumps(data, indent=4))  # inspect full response\n",
    "print(data[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efac9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'After the Honeymoon Table 3.jpg' for Table 3...\n",
      "  ❌ API request failed: HTTPConnectionPool(host='127.0.0.1', port=1234): Read timed out. (read timeout=300)\n",
      "Processing 'After the Honeymoon Table 4.jpg' for Table 4...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import base64\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "## ------------------- CONFIGURATION ------------------- ##\n",
    "# All settings are grouped here for easy access.\n",
    "\n",
    "# The local API endpoint\n",
    "API_URL = \"http://127.0.0.1:1234/v1/chat/completions\"\n",
    "\n",
    "# The vision-capable model you are using in LM Studio\n",
    "MODEL_NAME = \"gemma-3-vision\"\n",
    "\n",
    "# List of image files to process\n",
    "IMAGE_FILES = [\n",
    "    \"After the Honeymoon Table 3.jpg\",\n",
    "    \"After the Honeymoon Table 4.jpg\",\n",
    "    \"After the Honeymoon Table 5.jpg\",\n",
    "    \"Attractive Alternative Partners Table 1.jpg\",\n",
    "    \"Attractive Alternative Partners Table 3.jpg\",\n",
    "    \"Lucky Guy in Love Table 2.jpg\",\n",
    "    \"Lucky Guy in Love Table 3.jpg\"\n",
    "]\n",
    "\n",
    "# The output CSV filename\n",
    "OUTPUT_CSV = \"extracted_xyz_coordinates.csv\"\n",
    "\n",
    "## ------------------- CORE FUNCTIONS ------------------- ##\n",
    "\n",
    "def process_image(file_path: Path, table_no: str) -> list | None:\n",
    "    \"\"\"Encodes an image, sends it to the API, and parses the response.\"\"\"\n",
    "    print(f\"Processing '{file_path.name}' for Table {table_no}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Encode image to base64\n",
    "        with file_path.open(\"rb\") as f:\n",
    "            image_base64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error reading or encoding file: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Prepare prompts and payload for the API\n",
    "    system_prompt = \"You are an expert assistant specializing in extracting structured data from tables in images. Your output must be only a valid JSON array of objects, with no other text.\"\n",
    "    user_prompt = \"Extract all X, Y, and Z coordinates from this table. Format the output as a JSON array where each object has 'x', 'y', and 'z' keys. For example: `[{\\\"x\\\": 10, \\\"y\\\": -20, \\\"z\\\": 30}]`\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": user_prompt},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 3. Send request and handle potential errors\n",
    "    try:\n",
    "        response = requests.post(API_URL, json=payload, timeout=300)\n",
    "        response.raise_for_status()  # Raises an exception for bad status codes (4xx or 5xx)\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  ❌ API request failed: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"  ❌ Failed to decode JSON response from API.\")\n",
    "        return None\n",
    "\n",
    "    # 4. Safely extract model output and parse it\n",
    "    try:\n",
    "        model_output = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # The LLM sometimes wraps the JSON in markdown backticks\n",
    "        cleaned_output = re.sub(r'```json\\n?|```', '', model_output).strip()\n",
    "        coords = json.loads(cleaned_output)\n",
    "        \n",
    "        # Add the table number to each coordinate dictionary\n",
    "        for row in coords:\n",
    "            row[\"table_no\"] = table_no\n",
    "        \n",
    "        print(f\"  ✅ Successfully extracted {len(coords)} coordinate sets.\")\n",
    "        return coords\n",
    "    except (KeyError, IndexError):\n",
    "        print(\"  ❌ API response was missing expected 'choices' structure.\")\n",
    "        print(f\"  -> Raw response: {data}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"  ❌ Could not parse the model's output as JSON.\")\n",
    "        print(f\"  -> Model output: {model_output}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_to_csv(data: list, output_file: str):\n",
    "    \"\"\"Saves a list of dictionaries to a CSV file.\"\"\"\n",
    "    if not data:\n",
    "        print(\"No data was extracted, CSV file will not be created.\")\n",
    "        return\n",
    "\n",
    "    fieldnames = [\"table_no\", \"x\", \"y\", \"z\"]\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "    \n",
    "    print(f\"\\nExtraction complete. Data saved to '{output_file}'.\")\n",
    "\n",
    "\n",
    "## ------------------- MAIN EXECUTION ------------------- ##\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main script to orchestrate the extraction and saving process.\"\"\"\n",
    "    all_extracted_rows = []\n",
    "    \n",
    "    for file_name in IMAGE_FILES:\n",
    "        image_path = Path(file_name)\n",
    "        \n",
    "        # Skip files that don't exist\n",
    "        if not image_path.exists():\n",
    "            print(f\"⚠️  Warning: File not found, skipping: {file_name}\")\n",
    "            continue\n",
    "\n",
    "        # Extract table number from filename once\n",
    "        match = re.search(r\"Table\\s*(\\d+)\", file_name, re.IGNORECASE)\n",
    "        table_no = match.group(1) if match else \"unknown\"\n",
    "        \n",
    "        # Process the image and collect the results\n",
    "        coordinates = process_image(image_path, table_no)\n",
    "        if coordinates:\n",
    "            all_extracted_rows.extend(coordinates)\n",
    "            \n",
    "    # Save all collected data to a single CSV file\n",
    "    save_to_csv(all_extracted_rows, OUTPUT_CSV)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54005b48",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e787a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 1: EXTRACT METADATA FROM ALL HTML FILES\n",
    "## ----------------------------------------------------------------\n",
    "\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "extracted_metadata = []\n",
    "\n",
    "print(\"--- Starting Metadata Extraction ---\")\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Initialize variables for this file\n",
    "    title = None\n",
    "    pmid = None\n",
    "    pmcid = None\n",
    "    keywords = []\n",
    "    \n",
    "    # --- Extract Title ---\n",
    "    title_tag = soup.find('h1', class_='heading-title')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "\n",
    "    # --- Extract PMID and PMCID from the keywords meta tag ---\n",
    "    keywords_meta_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords_meta_tag:\n",
    "        content_str = keywords_meta_tag.get('content', '')\n",
    "        for part in content_str.split(','):\n",
    "            cleaned_part = part.strip()\n",
    "            if cleaned_part.startswith('pmid:'):\n",
    "                pmid = cleaned_part.replace('pmid:', '').strip()\n",
    "            elif cleaned_part.startswith('PMC'):\n",
    "                pmcid = cleaned_part\n",
    "    \n",
    "    # --- Extract Keywords from the body ---\n",
    "    keyword_strong_tag = soup.find('strong', string=re.compile(r'Keywords:', re.IGNORECASE))\n",
    "    if keyword_strong_tag:\n",
    "        next_element = keyword_strong_tag.next_sibling\n",
    "        if next_element and isinstance(next_element, NavigableString):\n",
    "            keyword_str = next_element.strip().rstrip('.')\n",
    "            keywords = [kw.strip() for kw in keyword_str.split(';')]\n",
    "\n",
    "    # Store all found data for this file\n",
    "    extracted_metadata.append({\n",
    "        'Title': title,\n",
    "        'PMID': pmid,\n",
    "        'PMCID': pmcid,\n",
    "        'Keywords': '; '.join(keywords) # Join list into a single string\n",
    "    })\n",
    "\n",
    "# Convert the extracted data into a DataFrame\n",
    "metadata_df = pd.DataFrame(extracted_metadata)\n",
    "print(\"\\n--- Metadata Extraction Complete ---\")\n",
    "print(\"Found the following articles:\")\n",
    "print(metadata_df[['Title', 'PMID']])\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 2: LOAD YOUR COORDINATE DATA\n",
    "## ----------------------------------------------------------------\n",
    "coords_df = pd.read_csv(\"coordinates.csv\")\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 3: MAP THE TITLES TO MERGE THE DATASETS\n",
    "## ----------------------------------------------------------------\n",
    "\n",
    "title_map = {\n",
    "    'After the Honeymoon': 'After the Honeymoon: Neural and Genetic Correlates of Romantic Love in Newlywed Marriages',\n",
    "    'Attractive Alternative Partners': 'Manipulation of Self-Expansion Alters Responses to Attractive Alternative Partners',\n",
    "    'Lucky Guy in Love': \"Decreased Empathic Responses to the 'Lucky Guy' in Love: The Effect of Intrasexual Competition\"\n",
    "}\n",
    "\n",
    "# Use the map to create a new 'Title' column in the coords_df for merging\n",
    "coords_df['Title'] = coords_df['Article'].map(title_map)\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 4: MERGE DATAFRAMES AND FINALIZE THE TABLE\n",
    "## ----------------------------------------------------------------\n",
    "# Merge the two dataframes using the 'Title' column as the key\n",
    "final_df = pd.merge(coords_df, metadata_df, on='Title')\n",
    "\n",
    "# Select and reorder columns to match your desired output\n",
    "final_df = final_df[[\n",
    "    'PMID',\n",
    "    'PMCID',\n",
    "    'Keywords',\n",
    "    'Table',\n",
    "    'X',\n",
    "    'Y',\n",
    "    'Z'\n",
    "]]\n",
    "\n",
    "print(\"\\n--- Final Merged Table ---\")\n",
    "print(final_df.to_string())\n",
    "\n",
    "# Optional: Save the final table to a new CSV file\n",
    "final_df.to_csv('final_merged_data.csv', index=False)\n",
    "print(\"\\n✅ Successfully saved the final table to 'final_merged_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
