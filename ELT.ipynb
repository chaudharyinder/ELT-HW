{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35f102c9",
   "metadata": {},
   "source": [
    "# Getting Query from PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c925d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading URL 1/3: https://pubmed.ncbi.nlm.nih.gov/27242579/...\n",
      "-> Saved C:\\Users\\User\\Desktop\\Psychoinformatics Neuroinformatics\\ETL_HW\\html_outputs\\output_1.html (134,094 bytes)\n",
      "\n",
      "Downloading URL 2/3: https://pubmed.ncbi.nlm.nih.gov/32457675/...\n",
      "-> Saved C:\\Users\\User\\Desktop\\Psychoinformatics Neuroinformatics\\ETL_HW\\html_outputs\\output_2.html (138,335 bytes)\n",
      "\n",
      "Downloading URL 3/3: https://pubmed.ncbi.nlm.nih.gov/32528365/...\n",
      "-> Saved C:\\Users\\User\\Desktop\\Psychoinformatics Neuroinformatics\\ETL_HW\\html_outputs\\output_3.html (128,509 bytes)\n",
      "\n",
      "All downloads complete.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# 1. Create a list of all the URLs you want to download\n",
    "URLS = [\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/27242579/\",\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/32457675/\",\n",
    "    \"https://pubmed.ncbi.nlm.nih.gov/32528365/\"\n",
    "]\n",
    "\n",
    "# Set browser-like headers to avoid being blocked by the site\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Referer\": \"https://pubmed.ncbi.nlm.nih.gov/\",\n",
    "}\n",
    "\n",
    "# Create a directory to store the output files\n",
    "output_dir = Path(\"html_outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# Use a single session for all requests for efficiency\n",
    "with requests.Session() as s:\n",
    "    s.headers.update(headers)\n",
    "    \n",
    "    # 2. Loop through each URL in the list\n",
    "    for i, url in enumerate(URLS):\n",
    "        print(f\"Downloading URL {i+1}/{len(URLS)}: {url[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # 3. Create a unique filename for each URL\n",
    "            outfile = output_dir / f\"output_{i+1}.html\"\n",
    "\n",
    "            resp = s.get(url, timeout=30)\n",
    "            resp.raise_for_status()  # raise an error for non-200 responses\n",
    "\n",
    "            # Use server-provided encoding when available; default to utf-8\n",
    "            if not resp.encoding:\n",
    "                resp.encoding = \"utf-8\"\n",
    "                \n",
    "            outfile.write_text(resp.text, encoding=resp.encoding)\n",
    "\n",
    "            print(f\"-> Saved {outfile.resolve()} ({outfile.stat().st_size:,} bytes)\\n\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"!! Failed to download URL {i+1}. Error: {e}\\n\")\n",
    "        \n",
    "        # Optional: Add a small delay to be respectful to the server\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"All downloads complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1277d0a",
   "metadata": {},
   "source": [
    "# Extracting PMID from the HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d5354c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 HTML files to process in 'html_outputs'...\n",
      "\n",
      "Processing: After the Honeymoon.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Processing: Attractive Alternative Partners.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Processing: Lucky Guy in Love.html\n",
      "  -> Found 1 PMIDs.\n",
      "\n",
      "Source File               | PMID\n",
      "--------------------------|--------\n",
      "After the Honeymoon.html  | 32457675\n",
      "Attractive Alternative Partners.html | 32528365\n",
      "Lucky Guy in Love.html    | 27242579\n",
      "\n",
      "========================================\n",
      "Processing complete!\n",
      "Total PMIDs extracted: 3\n",
      "['32457675', '32528365', '27242579']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define the directory containing your HTML files\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "\n",
    "# 2. Create a list to store the results as dictionaries\n",
    "extraction_results = []\n",
    "\n",
    "# 3. Find all .html files in the directory and loop through them\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "print(f\"Found {len(html_files)} HTML files to process in '{INPUT_DIR}'...\\n\")\n",
    "\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    \n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Find the specific meta tag\n",
    "    meta = soup.find('meta', attrs={'name': 'log_displayeduids'})\n",
    "    \n",
    "    if meta:\n",
    "        pmids_str = meta.get('content', '')\n",
    "        if pmids_str:\n",
    "            # Split the string by comma to get individual IDs\n",
    "            pmids = pmids_str.split(',')\n",
    "            \n",
    "            # For each PMID found, add it to our results with its source file\n",
    "            for pmid in pmids:\n",
    "                cleaned_pmid = pmid.strip()\n",
    "                if cleaned_pmid: # Ensure it's not an empty string\n",
    "                    extraction_results.append({\n",
    "                        'source_file': html_file.name,\n",
    "                        'pmid': cleaned_pmid\n",
    "                    })\n",
    "            \n",
    "            print(f\"  -> Found {len(pmids)} PMIDs.\\n\")\n",
    "        else:\n",
    "            print(\"  -> Meta tag found, but it has no content.\\n\")\n",
    "    else:\n",
    "        print(f\"  -> WARNING: Could not find the 'log_displayeduids' meta tag in this file.\\n\")\n",
    "\n",
    "## **Extraction Results**\n",
    "\n",
    "# 4. Print the final results directly to the console\n",
    "print(f\"{'Source File':<25} | {'PMID'}\")\n",
    "print(f\"{'-'*25}-|--------\")\n",
    "\n",
    "if not extraction_results:\n",
    "    print(\"No PMIDs were found.\")\n",
    "else:\n",
    "    for item in extraction_results:\n",
    "        print(f\"{item['source_file']:<25} | {item['pmid']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Total PMIDs extracted: {len(extraction_results)}\")\n",
    "extraction_results[:3]\n",
    "pmids_only = [item['pmid'] for item in extraction_results]\n",
    "print(pmids_only)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28ea673",
   "metadata": {},
   "source": [
    "# Extracting PMC ID from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "978bfa3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 HTML files to process in 'html_outputs'...\n",
      "\n",
      "Processing: After the Honeymoon.html\n",
      "  -> Found PMC ID: PMC7223160\n",
      "\n",
      "Processing: Attractive Alternative Partners.html\n",
      "  -> Found PMC ID: PMC7264388\n",
      "\n",
      "Processing: Lucky Guy in Love.html\n",
      "  -> Found PMC ID: PMC4863427\n",
      "\n",
      "Source File               | PMC ID\n",
      "--------------------------|-----------\n",
      "After the Honeymoon.html  | PMC7223160\n",
      "Attractive Alternative Partners.html | PMC7264388\n",
      "Lucky Guy in Love.html    | PMC4863427\n",
      "\n",
      "========================================\n",
      "Processing complete!\n",
      "Total files with PMC IDs: 3\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define the directory containing your HTML files\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "\n",
    "# 2. Create a list to store the results\n",
    "pmc_results = []\n",
    "\n",
    "# 3. Find all .html files in the directory and loop through them\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "print(f\"Found {len(html_files)} HTML files to process in '{INPUT_DIR}'...\\n\")\n",
    "\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    \n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Find the meta tag with name=\"keywords\"\n",
    "    keywords_meta_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    \n",
    "    pmc_id_found = False\n",
    "    if keywords_meta_tag:\n",
    "        # Get the content string, which contains pmid, pmcid, doi, etc.\n",
    "        content_str = keywords_meta_tag.get('content', '')\n",
    "        \n",
    "        # Split the string by commas to get individual parts\n",
    "        content_parts = content_str.split(',')\n",
    "        \n",
    "        # Loop through the parts to find the one starting with \"PMC\"\n",
    "        for part in content_parts:\n",
    "            # .strip() removes any leading/trailing whitespace\n",
    "            cleaned_part = part.strip()\n",
    "            if cleaned_part.startswith('PMC'):\n",
    "                pmc_results.append({\n",
    "                    'source_file': html_file.name,\n",
    "                    'pmc_id': cleaned_part\n",
    "                })\n",
    "                print(f\"  -> Found PMC ID: {cleaned_part}\\n\")\n",
    "                pmc_id_found = True\n",
    "                break # Stop searching once the PMC ID is found\n",
    "    \n",
    "    # If the loop finishes and no PMC ID was found\n",
    "    if not pmc_id_found:\n",
    "        print(f\"  -> No PMC ID found in this file.\\n\")\n",
    "\n",
    "## **Extraction Results**\n",
    "\n",
    "# 4. Print the final results directly to the console\n",
    "print(f\"{'Source File':<25} | {'PMC ID'}\")\n",
    "print(f\"{'-'*25}-|-----------\")\n",
    "\n",
    "if not pmc_results:\n",
    "    print(\"No PMC IDs were found in any of the files.\")\n",
    "else:\n",
    "    for item in pmc_results:\n",
    "        print(f\"{item['source_file']:<25} | {item['pmc_id']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Total files with PMC IDs: {len(pmc_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e9f08a",
   "metadata": {},
   "source": [
    "# Extracted Neuroimaging Coordinates using Gemini 2.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eadf79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully created coordinates.csv in your folder!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# The extracted data stored in a list of lists\n",
    "data = [\n",
    "    [\"After the Honeymoon\", 3, -6, -24, 57],\n",
    "    [\"After the Honeymoon\", 3, 15, -15, -12],\n",
    "    [\"After the Honeymoon\", 3, 54, 21, 3],\n",
    "    [\"After the Honeymoon\", 4, 6, -21, -21],\n",
    "    [\"After the Honeymoon\", 4, 3, -33, -21],\n",
    "    [\"After the Honeymoon\", 4, 39, -27, -9],\n",
    "    [\"After the Honeymoon\", 4, 15, -90, 3],\n",
    "    [\"After the Honeymoon\", 4, 45, -78, 24],\n",
    "    [\"After the Honeymoon\", 5, -3, -15, -21],\n",
    "    [\"After the Honeymoon\", 5, 0, 0, 23],\n",
    "    [\"After the Honeymoon\", 5, 3, 0, 24],\n",
    "    [\"Attractive Alternative Partners\", 1, -4, 22, 32],\n",
    "    [\"Attractive Alternative Partners\", 1, -4, 40, 18],\n",
    "    [\"Attractive Alternative Partners\", 1, 0, 26, 26],\n",
    "    [\"Attractive Alternative Partners\", 1, 2, 50, 0],\n",
    "    [\"Attractive Alternative Partners\", 3, 0, 64, 2],\n",
    "    [\"Attractive Alternative Partners\", 3, -56, -14, -4],\n",
    "    [\"Attractive Alternative Partners\", 3, 50, -2, -22],\n",
    "    [\"Attractive Alternative Partners\", 3, 2, -6, 6],\n",
    "    [\"Attractive Alternative Partners\", 3, 2, -90, 0],\n",
    "    [\"Attractive Alternative Partners\", 3, -20, 16, -4],\n",
    "    [\"Attractive Alternative Partners\", 3, -16, -26, -12],\n",
    "    [\"Attractive Alternative Partners\", 3, -26, -98, -2],\n",
    "    [\"Attractive Alternative Partners\", 3, 40, -10, -24],\n",
    "    [\"Attractive Alternative Partners\", 3, 28, -98, -10],\n",
    "    [\"Attractive Alternative Partners\", 3, -38, -20, -12],\n",
    "    [\"Attractive Alternative Partners\", 3, -36, -74, 56],\n",
    "    [\"Attractive Alternative Partners\", 3, -34, -78, 54],\n",
    "    [\"Attractive Alternative Partners\", 3, 6, -26, 22],\n",
    "    [\"Attractive Alternative Partners\", 3, -6, -30, 20],\n",
    "    [\"Lucky Guy in Love\", 2, 8, 12, 58],\n",
    "    [\"Lucky Guy in Love\", 2, -30, 22, 4],\n",
    "    [\"Lucky Guy in Love\", 2, 42, 10, 0],\n",
    "    [\"Lucky Guy in Love\", 2, -6, 14, 42],\n",
    "    [\"Lucky Guy in Love\", 2, -62, -22, 34],\n",
    "    [\"Lucky Guy in Love\", 2, 68, -24, 38],\n",
    "    [\"Lucky Guy in Love\", 2, 18, -70, -22],\n",
    "    [\"Lucky Guy in Love\", 2, 32, -94, -8],\n",
    "    [\"Lucky Guy in Love\", 2, 18, -88, 20],\n",
    "    [\"Lucky Guy in Love\", 2, 46, -60, 28],\n",
    "    [\"Lucky Guy in Love\", 2, -10, -56, 20],\n",
    "    [\"Lucky Guy in Love\", 2, 26, 28, 44],\n",
    "    [\"Lucky Guy in Love\", 2, 8, 42, -12],\n",
    "    [\"Lucky Guy in Love\", 2, 62, -4, -18],\n",
    "    [\"Lucky Guy in Love\", 2, -46, -78, 30],\n",
    "    [\"Lucky Guy in Love\", 2, 62, -4, -18],\n",
    "    [\"Lucky Guy in Love\", 3, -36, -12, 44],\n",
    "    [\"Lucky Guy in Love\", 3, -6, 10, 34],\n",
    "    [\"Lucky Guy in Love\", 3, -10, -26, 16],\n",
    "    [\"Lucky Guy in Love\", 3, -12, -54, 54],\n",
    "    [\"Lucky Guy in Love\", 3, 36, 12, 14],\n",
    "    [\"Lucky Guy in Love\", 3, 54, -58, 34],\n",
    "    [\"Lucky Guy in Love\", 3, 16, 36, 46],\n",
    "    [\"Lucky Guy in Love\", 3, 12, 54, 20],\n",
    "]\n",
    "\n",
    "# Define the column headers\n",
    "headers = [\"Article\", \"Table\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "# index=False prevents pandas from writing row indices into the file\n",
    "df.to_csv(\"coordinates.csv\", index=False)\n",
    "\n",
    "print(\"✅ Successfully created coordinates.csv in your folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56969471",
   "metadata": {},
   "source": [
    "# Extracting the keywords from HTMLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d0f278d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 HTML files to process in 'html_outputs'...\n",
      "\n",
      "Processing: After the Honeymoon.html\n",
      "  -> Found 5 keywords.\n",
      "\n",
      "Processing: Attractive Alternative Partners.html\n",
      "  -> Found 5 keywords.\n",
      "\n",
      "Processing: Lucky Guy in Love.html\n",
      "  -> Found 6 keywords.\n",
      "\n",
      "Source File               | Keywords\n",
      "--------------------------|-----------\n",
      "After the Honeymoon.html  | dopamine, fMRI, marriage, pair-bonds, romantic love\n",
      "Attractive Alternative Partners.html | attention to alternatives, close relationship, romantic love, self-expansion, social neuroscience\n",
      "Lucky Guy in Love.html    | AI, MPFC, aMCC, fMRI, intrasexual competition, pain empathy\n",
      "\n",
      "========================================\n",
      "Processing complete!\n",
      "Total files with keywords: 3\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# 1. Define the directory containing your HTML files\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "\n",
    "# 2. Create a list to store the results\n",
    "keyword_results = []\n",
    "\n",
    "# 3. Find all .html files and loop through them\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "print(f\"Found {len(html_files)} HTML files to process in '{INPUT_DIR}'...\\n\")\n",
    "\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    \n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Find the <strong> tag that contains the text \"Keywords:\"\n",
    "    # We use a regex with re.IGNORECASE to match \"Keywords:\" or \"keywords:\"\n",
    "    keyword_tag = soup.find('strong', string=re.compile(r'Keywords:', re.IGNORECASE))\n",
    "    \n",
    "    keywords_found = False\n",
    "    if keyword_tag:\n",
    "        # The keywords are in the text node immediately following the <strong> tag\n",
    "        next_element = keyword_tag.next_sibling\n",
    "        \n",
    "        # Check if the next element is actually text (a NavigableString)\n",
    "        if next_element and isinstance(next_element, NavigableString):\n",
    "            # .strip() removes whitespace, .rstrip('.') removes the final period\n",
    "            keyword_str = next_element.strip().rstrip('.')\n",
    "            \n",
    "            # Split the string by semicolon and clean up each keyword\n",
    "            keywords = [kw.strip() for kw in keyword_str.split(';')]\n",
    "            \n",
    "            keyword_results.append({\n",
    "                'source_file': html_file.name,\n",
    "                'keywords': keywords\n",
    "            })\n",
    "            print(f\"  -> Found {len(keywords)} keywords.\\n\")\n",
    "            keywords_found = True\n",
    "\n",
    "    if not keywords_found:\n",
    "        print(f\"  -> No keywords section found in this file.\\n\")\n",
    "\n",
    "## **Extraction Results**\n",
    "\n",
    "# 4. Print the final results\n",
    "print(f\"{'Source File':<25} | {'Keywords'}\")\n",
    "print(f\"{'-'*25}-|-----------\")\n",
    "\n",
    "if not keyword_results:\n",
    "    print(\"No keywords were found in any of the files.\")\n",
    "else:\n",
    "    for item in keyword_results:\n",
    "        # ', '.join() converts the list of keywords into a nice string for printing\n",
    "        keywords_str = ', '.join(item['keywords'])\n",
    "        print(f\"{item['source_file']:<25} | {keywords_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Processing complete!\")\n",
    "print(f\"Total files with keywords: {len(keyword_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da242083",
   "metadata": {},
   "source": [
    "# Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bebe21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Metadata Extraction ---\n",
      "Processing: After the Honeymoon.html\n",
      "Processing: Attractive Alternative Partners.html\n",
      "Processing: Lucky Guy in Love.html\n",
      "\n",
      "--- Metadata Extraction Complete ---\n",
      "Found the following articles:\n",
      "                                               Title      PMID\n",
      "0  After the Honeymoon: Neural and Genetic Correl...  32457675\n",
      "1  Manipulation of Self-Expansion Alters Response...  32528365\n",
      "2  Decreased Empathic Responses to the 'Lucky Guy...  27242579\n",
      "\n",
      "--- Final Merged Table ---\n",
      "        PMID       PMCID                                                                                           Keywords  Table   X   Y   Z\n",
      "0   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      3  -6 -24  57\n",
      "1   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      3  15 -15 -12\n",
      "2   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      3  54  21   3\n",
      "3   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      4   6 -21 -21\n",
      "4   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      4   3 -33 -21\n",
      "5   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      4  39 -27  -9\n",
      "6   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      4  15 -90   3\n",
      "7   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      4  45 -78  24\n",
      "8   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      5  -3 -15 -21\n",
      "9   32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      5   0   0  23\n",
      "10  32457675  PMC7223160                                                dopamine; fMRI; marriage; pair-bonds; romantic love      5   3   0  24\n",
      "11  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      1  -4  22  32\n",
      "12  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      1  -4  40  18\n",
      "13  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      1   0  26  26\n",
      "14  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      1   2  50   0\n",
      "15  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3   0  64   2\n",
      "16  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3 -56 -14  -4\n",
      "17  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3  50  -2 -22\n",
      "18  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3   2  -6   6\n",
      "19  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3   2 -90   0\n",
      "20  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3 -20  16  -4\n",
      "21  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3 -16 -26 -12\n",
      "22  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3 -26 -98  -2\n",
      "23  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3  40 -10 -24\n",
      "24  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3  28 -98 -10\n",
      "25  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3 -38 -20 -12\n",
      "26  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3 -36 -74  56\n",
      "27  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3 -34 -78  54\n",
      "28  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3   6 -26  22\n",
      "29  32528365  PMC7264388  attention to alternatives; close relationship; romantic love; self-expansion; social neuroscience      3  -6 -30  20\n",
      "30  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2   8  12  58\n",
      "31  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2 -30  22   4\n",
      "32  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  42  10   0\n",
      "33  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  -6  14  42\n",
      "34  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2 -62 -22  34\n",
      "35  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  68 -24  38\n",
      "36  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  18 -70 -22\n",
      "37  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  32 -94  -8\n",
      "38  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  18 -88  20\n",
      "39  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  46 -60  28\n",
      "40  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2 -10 -56  20\n",
      "41  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  26  28  44\n",
      "42  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2   8  42 -12\n",
      "43  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  62  -4 -18\n",
      "44  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2 -46 -78  30\n",
      "45  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      2  62  -4 -18\n",
      "46  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3 -36 -12  44\n",
      "47  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3  -6  10  34\n",
      "48  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3 -10 -26  16\n",
      "49  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3 -12 -54  54\n",
      "50  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3  36  12  14\n",
      "51  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3  54 -58  34\n",
      "52  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3  16  36  46\n",
      "53  27242579  PMC4863427                                        AI; MPFC; aMCC; fMRI; intrasexual competition; pain empathy      3  12  54  20\n",
      "\n",
      "✅ Successfully saved the final table to 'final_merged_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 1: EXTRACT METADATA FROM ALL HTML FILES\n",
    "## ----------------------------------------------------------------\n",
    "\n",
    "INPUT_DIR = Path(\"html_outputs\")\n",
    "html_files = list(INPUT_DIR.glob(\"*.html\"))\n",
    "extracted_metadata = []\n",
    "\n",
    "print(\"--- Starting Metadata Extraction ---\")\n",
    "for html_file in html_files:\n",
    "    print(f\"Processing: {html_file.name}\")\n",
    "    html_text = html_file.read_text(encoding='utf-8', errors='ignore')\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    \n",
    "    # Initialize variables for this file\n",
    "    title = None\n",
    "    pmid = None\n",
    "    pmcid = None\n",
    "    keywords = []\n",
    "    \n",
    "    # --- Extract Title ---\n",
    "    title_tag = soup.find('h1', class_='heading-title')\n",
    "    if title_tag:\n",
    "        title = title_tag.get_text(strip=True)\n",
    "\n",
    "    # --- Extract PMID and PMCID from the keywords meta tag ---\n",
    "    keywords_meta_tag = soup.find('meta', attrs={'name': 'keywords'})\n",
    "    if keywords_meta_tag:\n",
    "        content_str = keywords_meta_tag.get('content', '')\n",
    "        for part in content_str.split(','):\n",
    "            cleaned_part = part.strip()\n",
    "            if cleaned_part.startswith('pmid:'):\n",
    "                pmid = cleaned_part.replace('pmid:', '').strip()\n",
    "            elif cleaned_part.startswith('PMC'):\n",
    "                pmcid = cleaned_part\n",
    "    \n",
    "    # --- Extract Keywords from the body ---\n",
    "    keyword_strong_tag = soup.find('strong', string=re.compile(r'Keywords:', re.IGNORECASE))\n",
    "    if keyword_strong_tag:\n",
    "        next_element = keyword_strong_tag.next_sibling\n",
    "        if next_element and isinstance(next_element, NavigableString):\n",
    "            keyword_str = next_element.strip().rstrip('.')\n",
    "            keywords = [kw.strip() for kw in keyword_str.split(';')]\n",
    "\n",
    "    # Store all found data for this file\n",
    "    extracted_metadata.append({\n",
    "        'Title': title,\n",
    "        'PMID': pmid,\n",
    "        'PMCID': pmcid,\n",
    "        'Keywords': '; '.join(keywords) # Join list into a single string\n",
    "    })\n",
    "\n",
    "# Convert the extracted data into a DataFrame\n",
    "metadata_df = pd.DataFrame(extracted_metadata)\n",
    "print(\"\\n--- Metadata Extraction Complete ---\")\n",
    "print(\"Found the following articles:\")\n",
    "print(metadata_df[['Title', 'PMID']])\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 2: LOAD YOUR COORDINATE DATA\n",
    "## ----------------------------------------------------------------\n",
    "coords_df = pd.read_csv(\"coordinates.csv\")\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 3: MAP THE TITLES TO MERGE THE DATASETS\n",
    "## ----------------------------------------------------------------\n",
    "\n",
    "title_map = {\n",
    "    'After the Honeymoon': 'After the Honeymoon: Neural and Genetic Correlates of Romantic Love in Newlywed Marriages',\n",
    "    'Attractive Alternative Partners': 'Manipulation of Self-Expansion Alters Responses to Attractive Alternative Partners',\n",
    "    'Lucky Guy in Love': \"Decreased Empathic Responses to the 'Lucky Guy' in Love: The Effect of Intrasexual Competition\"\n",
    "}\n",
    "\n",
    "# Use the map to create a new 'Title' column in the coords_df for merging\n",
    "coords_df['Title'] = coords_df['Article'].map(title_map)\n",
    "\n",
    "\n",
    "## ----------------------------------------------------------------\n",
    "## STEP 4: MERGE DATAFRAMES AND FINALIZE THE TABLE\n",
    "## ----------------------------------------------------------------\n",
    "# Merge the two dataframes using the 'Title' column as the key\n",
    "final_df = pd.merge(coords_df, metadata_df, on='Title')\n",
    "\n",
    "# Select and reorder columns to match your desired output\n",
    "final_df = final_df[[\n",
    "    'PMID',\n",
    "    'PMCID',\n",
    "    'Keywords',\n",
    "    'Table',\n",
    "    'X',\n",
    "    'Y',\n",
    "    'Z'\n",
    "]]\n",
    "\n",
    "print(\"\\n--- Final Merged Table ---\")\n",
    "print(final_df.to_string())\n",
    "\n",
    "# Optional: Save the final table to a new CSV file\n",
    "final_df.to_csv('final_merged_data.csv', index=False)\n",
    "print(\"\\n✅ Successfully saved the final table to 'final_merged_data.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
